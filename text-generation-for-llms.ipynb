{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Generation using LLama Model\n\n### This Task a straightforward way to utilize the LLaMA model for generating text based on a given prompt. By leveraging the capabilities of the Hugging Face Transformers library, users can easily load pre-trained models, encode inputs, generate text, and decode the outputs. This approach is applicable in various natural language processing tasks, including storytelling, dialogue generation, and more. The flexibility of the model allows for adjustments in prompt and generation parameters to tailor the output to specific needs.\n\n<figure>\n        <img src=\"https://debuggercafe.com/wp-content/uploads/2023/11/Text-Generation-with-Transformers-e1700010212188.png\" alt =\"Audio Art\" style='width:800px;height:500px;'>\n        <figcaption>","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\n# Load pre-trained model and tokenizer\nmodel_name = \"openlm-research/open_llama_3b\"  # You can use other LLaMA models as available\nmodel = LlamaForCausalLM.from_pretrained(model_name)\ntokenizer = LlamaTokenizer.from_pretrained(model_name)\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define the prompt\nprompt = \"Once upon a time in a faraway land, there was a small village where people lived in harmony with nature. One day,\"\n\n# Encode the input prompt\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n# Generate text\nmax_length = 200  # Adjust the length of the generated text as needed\noutputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the generated text\nprint(\"Generated Text:\")\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T11:36:17.520487Z","iopub.execute_input":"2024-08-06T11:36:17.520914Z","iopub.status.idle":"2024-08-06T11:39:32.539614Z","shell.execute_reply.started":"2024-08-06T11:36:17.520884Z","shell.execute_reply":"2024-08-06T11:39:32.538360Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c93399a7d794cffad5ea7464ddb5a96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0feece5337384f5da5c6abe543c8ba05"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded67f981f5d4383a6915ea307e74fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2412295cf9e49a9bf9414a6c668b9a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ee4f89cdb7b4c1a9fcd74e891803cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5083ef1ed5a848bc82be5cb35e8ed281"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"},{"name":"stdout","text":"Generated Text:\nOnce upon a time in a faraway land, there was a small village where people lived in harmony with nature. One day, a group of greedy men came to the village and said, “We want to build a dam and flood your village. We will give you a lot of money to move to a new place.” The people of the village were very happy to move to a new place, but they were sad to leave their old home.\nThe greedy men built the dam and flooded the village. The people of the village were sad and angry. They said, “We want to go back to our old home. We want to rebuild our village.” The greedy men said, “No, you can’t go back. You have to move to a new place.” The people of the village were very sad and angry. They said, “We want to go back to our old home. We want to rebuild our village.” The greedy men said, “\n","output_type":"stream"}]}]}